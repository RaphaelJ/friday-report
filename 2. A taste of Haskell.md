# A taste of Haskell

*Haskell* is a functional and strongly typed programming language.

The first part of this section explains that functional programming is mostly
about programming without mutation and side effect.

The second part gives a brief introduction to *Haskell* syntax and type system.

## What is functional programming

Functional programming is, in its most universal definition, a programming style
which tries to *avoid mutating state* and mutable data structures, by
*emphasing on the composition of expressions* in place of the execution of
commands and instructions.

See the following C imperative program which manipulates a rectangle:

```C
struct rect_t {
    int x, y, width, height;
}

void move(rect_t *rect, int dx, int dy)
{
    rect->x += dx;
    rect->y += dy;
}

rect_t my_rectangle(void)
{
    rect_t rect = {
        .x     = 0,   .y      = 0,
        .width = 100, .height = 100
    };

    move(&rect, 10, 10);

    return rect;
}
```

And now compare it with this other C program written in a functional style:

```C
struct rect_t {
    int x, y, width, height;
}

rect_t move(rect_t rect, int dx, int dy)
{
    return {
        .x      = rect.x + dx,
        .y      = rect.y + dy,
        .width  = rect.width,
        .height = rect.height
    };
}

rect_t my_rectangle()
{
    rect_t rect = {
        .x     = 0,   .y      = 0,
        .width = 100, .height = 100
    };

    return move(rect, 10, 10);
}
```

The main difference is that in the functional style, *we never modify
any data previously created*. In our example, when the original rectangle needs
to be moved a *new* rectangle will be created. In functional programming there
is no *destructive update* and data are *immutable*.

With this absence of destructive updates, variables are used in a mathematical
sense, as identifiers to immutable expressions or values. In the `my_rectangle`
function of the example, `rect` is associated to the value of an expression, 
and this value will never change.

Functions which are written in this functional style, without using any
mutable state, are called *pure* and are very close to the mathematical
definition of a function. Pure functions accepts a set of immutable values as
input and computes a new immutable value as output, doing nothing else.

### Removing mutable states does matter

Prohibiting mutable states has the effect of *removing all side effects*.
Pure functions can't call `printf()` or write to a file, as this can be seen
as modifying the state of the runtime environment. Nor they can read from a
file this is equivalent to read from a mutable state.

*Pure functions are deterministic*. A pure function call can be replaced by the
value it will return. This is called *referential transparency*. Referential
transparency is the key concept to understand why *friday* and the *Haskell*
compiler are able to safely fuse two image transformations into a single loop 
and to automatically paralellize image processing algorithms.

While time is important in imperative programming (it determines the order in
which instructions are executed), it is an implicit concept in functional
programming.
As pure expressions always return the same values, the time at which they are
executed does not matter. In *Haskell*, expressions and function calls are
evaluated in a dependency graph: think about a spreadsheet, you do not define
the order in which cells are computed, they are evaluated as needed.

Pure functions are also easier to test, thread-safe by design and more
reusable. Code written in a functional style is usualy easier to reason about as
you only need to find the definition of a variable to get its actual value
(this doesn't hold in an imperative program as values change over time).

### The world needs side effects

A sharp-minded reader will argue that a program that does not do any side
effect is useless, as it has no way to communicate with the outside world. And
indeed, programmers write programs for their side effects.

*Haskell* allows you to write functions which do side effects but they will
be tagged differently by the type system. Here is also the idea of
*contamination*: any function which call a function tagged as having side
effects will be tagged as well. Similary, a function which has been typed as not
having side effect will not be able to call a function with side effects.

Good functional programmers will try to write as few functions with side
effects as possible, to benefit from the advantages of pure functions.

## Haskell

Now that you know what functional programming is about, let me introduce
*Haskell*.

You don't need to know a lot of things about the language to understand the
remaining of this document.

From a syntactic point of view, *Haskell* programs are made out of
*equations*, not *instructions*. An equation has a left hand side and a
right hand side, with an `=` sign in between. The left hand side is the
identifier of the equation while the right hand side is an expression 
denoting its value.

```Haskell
-- This is a comment

x = y * z + 5
y = 10
z = 15
```

As you can see, equations don't need to be defined in order. The language
evaluates them when their values are needed.

Equations can accept arguments, in which case they are functions. There is no
parenthesis nor comma between function parameters. Likewise, there is no
parenthesis nor comma between arguments when making a function call.
Parenthesis are needed when the order of operations does not fit, like in
arithmetic. Function call is the "operator" with the highest priority. You
don't need to worry too much about that, it feels natural:

```Haskell
-- This function squares a number.
square n = n * n

-- This function accepts two numbers and computes the hypotenuse of a
-- triangle.
pythagoras a b = sqrt (square a + square b)
```

### Haskell's type system

While I told you several times that *Haskell* has a strong and advanced type
system, I didn't write any type nor function signature in the previous
examples.

This may feel strange at first, but these examples were actually entirely
statically typed. *Haskell* has a program-wise *type inference* engine. It's is
a feature which makes the compiler able to deduce expression, function and
equation types by itself.

We can ask the type inference engine for the type signatures he find while
analysing our equations. For example, if we ask the type signature of the
`pythagoras` function, we will get:

```Haskell
pythagoras :: Floating a
           => a -> a -> a
```

What we see on the second line is that the function accepts two values of type
`a` and that it returns a value of this *same* type `a`. Argument types are
separated by the `->` operator and the type at the right of the last `->` is
always the one of the returned value [1].

`a` is not a "real" type. It's a *type variable*. Type variables in
signatures are "free types" in the sense that they can be replaced by a "real"
type. If we were in *C++* or *Java* and if we were using templates or generics,
type variables would be called *type parameters* (the `T` in `vector<T>`).

In *Haskell*, type variables always start with a lower case letter while
"real" types start with an upper case letter. `Int` is a real type while
`a_type` is a type variable.

Constraints can be applied to type variables. Constraints in a type signature
are located after the `::` operator but before the `=>` operator. Our
`pythagoras` function has one constraint, `Floating a`. This constraint is
saying that our type variable `a` must be an instance of the `Floating` *type
class*. Type classes are similar to *Java* interfaces or *C++* abstract 
classes. This `Floating` type class is implemented by floating point numbers.

To summarize, this signature is telling us that our `pythagoras` function
accepts two floating point numbers of the same type and that it returns a
floating point number of this very same type. The type inference engine always
tries to generalize signatures as much as possible.

This kind of polymorphism which relies on type classes as constraints is called
*parametric polymorphism* and is similar to what can be done with *Java*
generics or *C++* templates. It's however very different from the traditional
Object Oriented polymorphism in the sense that every function call can be
resolved at compile time (there is *no late binding*). This is an important
feature for this project as you can write generic algorithms which don't suffer
from reduced performances when compared to their specialized counterparts.

[1] There is a practical reason for why function argument types are separated
by this `->` operator and not another, but this goes out of the scope of this
report. Take a look after *Currying* on the Internet if you are interested.
